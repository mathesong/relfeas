---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```
# relfeas

The goal of relfeas is to allow researchers to use reliability reported in test-retest studies to make assessments of the feasibility of new study designs. This study accompanies the preprint: We need to talk about reliability: Making better use of test-retest studies for study design and interpretation.

The package is still a bit rough around the edges on some functions.  It should be considered a work in progress.


## Installation

You can install relfeas from github with:

```{r gh-installation, eval = FALSE}
# install.packages("devtools")
devtools::install_github("mathesong/relfeas")
```


### Examples from the paper

Below, I detail the calculations performed in the manuscript, and show the code used from the package.  Please refer to the manuscript for the context

```{r, message=F, warning=F}
library(relfeas)
library(tidyverse)
library(ggplot2)
library(gridExtra)
library(pwr)
library(knitr)
```


#### Example 1

```{r}
sd2extrapRel(sd=0.32, icc_original = 0.32,
              sd_original = 0.10)

sd2extrapRel(sd = 0.32,icc_original =  0.32,
              sd_original =  0.10, rho = 2)
```


#### Example 2

```{r}
r_attenuation(0.8, 0.7)

pwr::pwr.r.test(r=sqrt(0.3), power=0.8)

pwr::pwr.r.test(r=sqrt(0.3)*r_attenuation(0.8, 0.7), power=0.8)
```


#### Example 3

```{r}
meanbp <- 1.91
delta_mean <- 0.12
delta_sd <- 0.1

sem <- icc2sem(icc = 0.8, sd =  0.22)
sd = delta_sd*(meanbp)

(delta_icc <- sem2icc(sem*2, sd))

samplesize <- 2

(sdd_indiv <- 100*((sem*1.96*sqrt(2))/meanbp))

(sdd_group <- 100*(((sem/sqrt(samplesize))*1.96*sqrt(2))/meanbp))

(power_n_within <- pwr::pwr.t.test(d=-12/7.3, sig.level = 0.05,
                                   type = "paired", alternative = "less",
                                   power=0.8))


ss_total <- sumStat_total(n1 = 20, mean1 =  delta_mean*meanbp, sd1 = delta_sd*meanbp,
              n2 = 20, mean2 = 2.5*delta_mean*meanbp, sd2=delta_sd*meanbp)

(delta_icc_patcntrl <- sem2icc(sem*2, ss_total$sd_total))

(d_true <- ss_total$d)

(d_meas <- d_attenuation(rel_total = delta_icc_patcntrl, d = d_true))

(pwr_increase_unpaired <- pwr::pwr.t.test(d=d_meas, power = 0.8, alternative = "greater")$n /
  pwr::pwr.t.test(d=d_true, power = 0.8, alternative = "greater")$n)

(pwr_increase_paired <- pwr::pwr.t.test(d=d_meas, power = 0.8, type = "paired", alternative = "greater")$n /
  pwr::pwr.t.test(d=d_true, type = "paired", power = 0.8)$n)
```


#### Example 4

```{r}
tspo_hab_10es <- 10/42
ser1b_10es <- 10/6

(tspo_n <- round(pwr::pwr.t.test(d = tspo_hab_10es, power = 0.8)$n))

(ser1b_n <- round(pwr::pwr.t.test(d = ser1b_10es, power = 0.8)$n))
```


#### Example 5

Note that the figure below is inspired by the work of Kristoffer Magnusson, a.k.a. R Psychologist, and his [Cohen's D interactive figure](http://rpsychologist.com/d3/cohend/) as well as his description of where [Cohen was wrong about overlap](http://rpsychologist.com/cohen-d-proportion-overlap).

```{r EffectSizes, fig.width = 10, fig.height = 3}
(sd_basic <- extrapRel2sd(0.7, 0.5))
(d_basic <- sdtot2mean2(sd_total = sd_basic, n1 = 20, n2=20, mean1 = 1))
(ol_basic <- d2overlap(d_basic$d))

(sd_acceptable <- extrapRel2sd(0.8, 0.5))
(d_acceptable <- sdtot2mean2(sd_total = sd_acceptable, n1 = 20, n2=20, mean1 = 1))
(ol_acceptable <- d2overlap(d_acceptable$d))

(sd_clin <- extrapRel2sd(0.9, 0.5))
(d_clin <- sdtot2mean2(sd_total = sd_clin, n1 = 20, n2=20, mean1 = 1))
(ol_clin <- d2overlap(d_clin$d))

graphtheme <- theme(axis.text.x=element_blank(),
                    axis.text.y=element_blank())

custcolours <- c('#85d4e3', '#e39f85')

d_fig <- grid.arrange(
  plot_difference(d = d_basic$d, colours=custcolours) + 
    labs(x='Lowest Acceptable Reliability (0.7)', title=NULL, y='Probability') + 
    graphtheme,
  plot_difference(d = d_acceptable$d, colours=custcolours) + 
    labs(x='Acceptable Reliability (0.8)', title=NULL, y='Probability') + 
    graphtheme,
  plot_difference(d = d_clin$d, colours=custcolours) + 
    labs(x='Clinical Reliability (0.9)', title=NULL, y='Probability') + 
    graphtheme,
  nrow=1)
```


### Other Examples

Not all the functions are used in the paper.  Here a demonstrate a couple of others.

#### Effect Size Comparisons

It is extremely important to consider effect sizes when performing study design.  For this purpose, I have included plotting functions such as in example 5 above.  I have also included a conversion function for Cohen's D alternatives for converting between them if one feels more intuitive than another.



```{r}
## If Cohen's D feels more intuitive for some application
es_convert(d = 1)

## If the Common Language Effect Size feels more intuitive for another application
es_convert(cles = 0.8)
```

#### Performing test-retest analysis

I have also included a test-retest analysis function which includes all the usual metrics which are reported in PET studies, as well as a few others which are useful.  It also produces a tidy output, so it can be used with tidy data within R pipelines.  I will use some data from the *agRee* package.

First we prepare the data into tidy format

```{r}
data("petVT", package = 'agRee')

details <- map(petVT, ~nrow(.)) %>% 
  as.data.frame()

Region <- c(rep(names(details[1]), details[1]),
            rep(names(details[2]), details[2]),
            rep(names(details[3]), details[3]))

petVT <- do.call(rbind, petVT) %>% 
  as_tibble() %>% 
  rename(Measurement1=V1, Measurement2=V2) %>% 
  mutate(Region = Region) %>% 
  mutate(Participant = 1:n()) %>% 
  gather(Measurement, Outcome, -Participant, -Region)
```

Now the data looks as follows:

```{r}
head(petVT)
```



Then we perform a test-retest analysis.  




```{r}
calc_trt <- petVT %>% 
  group_by(Region) %>% 
  nest() %>% 
  mutate(outcomes = map(data, ~ trt( as.data.frame(.x), values='Outcome', 
                                     cases = 'Participant', rater = 'Measurement' )))

trt_out <- map_df(calc_trt$outcomes, 'tidy') %>% 
  mutate(Region = calc_trt$Region)

kable(trt_out, digits = 2)
```



The table lists the following for each sample:

##### Distribution
* __mean__: Mean 
* __sd__: Standard deviation
* __cov__: Coefficient of variation 
* __skew__: Skew
* __kurtosis__: Kurtosis

##### Reliability
* __icc__: ICC
* __icc_l__: Lower bound of the 95% confidence interval of ICC
* __icc_u__: Upper bound of the 95% confidence interval of ICC



##### Measurement Imprecision
* __wscv__: Within-subject coefficient of variation: relative imprecision
* __sdd__: Smallest detectable difference: absolute imprecision (95% confidence interval)

##### Variation
* __absvar__: Absolute variability: the average absolute percentage change between measurements within individuals
* __signvar__: Signed variability: same as above but signed. This is a check for bias between measurements
* __signvar_sd__: Standard deviation of the signed variance values. This is useful for power analysis for within-subjects designs.
